{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "FbdwAb6YULXo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8guPwJuyCh42"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import json\n",
        "import pickle\n",
        "import gensim\n",
        "import gensim.downloader as api\n",
        "import torchtext\n",
        "from torch import nn, stack, tensor\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "jNUsluvRUQ6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPQEaxwmGy_N",
        "outputId": "06ff9238-4dc2-4e69-c258-20989b4ec1fe"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/My Drive/minivqaiust/image_features.pickle\", 'rb') as f:\n",
        "    image_feature = pickle.load(f)\n",
        "\n",
        "with open(\"/content/drive/My Drive/minivqaiust/image_question.json\") as f:\n",
        "      image_question_map = json.load(f)"
      ],
      "metadata": {
        "id": "u5e51vjEFIqq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions={}\n",
        "\n",
        "for image_id, question in image_question_map.items():\n",
        "  for q in question:\n",
        "    questions[q[0]] = {'question_text':q[1], 'image_id': str(image_id)}"
      ],
      "metadata": {
        "id": "u9cks--RF-RZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pre proccessing train data\n",
        "df = pd.read_csv(\"/content/drive/My Drive/minivqaiust/train.csv\")\n",
        "\n",
        "train_label=torch.tensor(list(df['label']))\n",
        "train_question=[questions[i]['question_text'] for i in list(df['question_id'])]\n",
        "train_image_feature = [image_feature[questions[i]['image_id']] for i in list(df['question_id'])]"
      ],
      "metadata": {
        "id": "Lb9GSMrnUWyq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pre proccessing validation data\n",
        "df = pd.read_csv(\"/content/drive/My Drive/minivqaiust/val.csv\")\n",
        "\n",
        "valid_label=torch.tensor(list(df['label']))\n",
        "valid_question=[questions[i]['question_text'] for i in list(df['question_id'])]\n",
        "valid_image_feature = torch.tensor([image_feature[questions[i]['image_id']] for i in list(df['question_id'])])"
      ],
      "metadata": {
        "id": "tSHUf1qfJJfZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0b4654f-e4cd-41f2-e2cd-32cc37e34eac"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-4e22ac001af4>:6: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
            "  valid_image_feature = torch.tensor([image_feature[questions[i]['image_id']] for i in list(df['question_id'])])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pre proccessing test data\n",
        "df = pd.read_csv(\"/content/drive/My Drive/minivqaiust/test.csv\")\n",
        "\n",
        "test_question=[questions[i]['question_text'] for i in list(df['question_id'])]\n",
        "test_image_feature = torch.tensor([image_feature[questions[i]['image_id']] for i in list(df['question_id'])])"
      ],
      "metadata": {
        "id": "3BaYh6IpJKCd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Embedding"
      ],
      "metadata": {
        "id": "ftLNcNguLmuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize\n",
        "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
        "\n",
        "# pre embedding model\n",
        "pre_model = api.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "id": "Y6euHxsSLpB1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding layer\n",
        "word_embedding = nn.Embedding(len(pre_model.index_to_key) + 1, len(pre_model.get_vector('hi')))"
      ],
      "metadata": {
        "id": "ZizvAQmtNND1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(input):\n",
        "  return [pre_model.get_index(token, default=-1) + 1 for token in tokenizer(input)]\n",
        "\n",
        "def padify(input):\n",
        "  encoded_input = [encode(x) for x in input]\n",
        "  return stack([nn.functional.pad(tensor(e),(0,15-len(e)),mode='constant',value=0) for e in encoded_input])"
      ],
      "metadata": {
        "id": "u3Bq0d3ZO5D3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# word embedding on train data\n",
        "with torch.no_grad():\n",
        "  train_question_embedding = word_embedding(padify(train_question))"
      ],
      "metadata": {
        "id": "5GzFKGWRO7sN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# word embedding on validation data\n",
        "with torch.no_grad():\n",
        "  valid_question_embedding = word_embedding(padify(valid_question))"
      ],
      "metadata": {
        "id": "fUs0G6nfRREx"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# word embedding on test data\n",
        "with torch.no_grad():\n",
        "  test_question_embedding = word_embedding(padify(test_question))"
      ],
      "metadata": {
        "id": "dbTa7eH5RR4L"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data loader\n"
      ],
      "metadata": {
        "id": "hK7YWW5oRrp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = torch.utils.data.TensorDataset(train_question_embedding, torch.tensor(train_image_feature), train_label)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "ntjQN-rjRvh_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_dataset = torch.utils.data.TensorDataset(valid_question_embedding, valid_image_feature, valid_label)\n",
        "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "O0ld9shNRvcq"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "LorGRwwUS62t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VQA(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(type(self), self).__init__()\n",
        "        self.lstm = nn.LSTM(256, 512, num_layers=1)\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(8192, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Linear(128, 10),\n",
        "            nn.BatchNorm1d(10),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, text, image):\n",
        "        text_features= torch.flatten(self.lstm(text)[0], start_dim=1)\n",
        "        input = torch.cat([text_features, image], dim=1)\n",
        "        logits = nn.functional.softmax(self.linear(input), dim=1)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "PsLh9eXGS22Z"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VQA()"
      ],
      "metadata": {
        "id": "PioQ-viZGI1C"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and Validation"
      ],
      "metadata": {
        "id": "PspgoaPwG0XD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0004)"
      ],
      "metadata": {
        "id": "kIlaF0DJG1yp"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validation_loop(model, dataloader):\n",
        "  size = len(dataloader.dataset)\n",
        "  correct = 0\n",
        "  avg_loss = 0\n",
        "  for batch, (text, image, y) in enumerate(dataloader):\n",
        "    prediction = model(text, image)\n",
        "    loss = loss_function(prediction, y)\n",
        "\n",
        "    output = [torch.argmax(p).item() for p in prediction]\n",
        "    correct += (torch.FloatTensor(output) == y).float().sum()\n",
        "    avg_loss += loss.item()\n",
        "\n",
        "  accuracy = correct / len(dataloader.dataset)\n",
        "  return avg_loss, accuracy\n"
      ],
      "metadata": {
        "id": "szrILLfGM0eV"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader, model, loss_function, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    correct = 0\n",
        "    avg_loss = 0\n",
        "    for batch, (text, image, label) in enumerate(dataloader):\n",
        "        prediction = model(text, image)\n",
        "        loss = loss_function(prediction, label)\n",
        "        avg_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        output = [torch.argmax(p).item() for p in prediction]\n",
        "        correct += (torch.FloatTensor(output) == y).float().sum()\n",
        "\n",
        "    avg_loss /= (len(dataloader.dataset) // 64 + 1)\n",
        "    accuracy = correct / len(dataloader.dataset)\n",
        "\n",
        "    val_loss, val_acc = validation_loop(model, valid_dataloader)\n",
        "    print(f\"training / loss: {avg_loss:>7f} | accuracy: {accuracy}\")\n",
        "    print(f\"val / loss: {val_loss:>7f} | accuracy: {val_acc}\")"
      ],
      "metadata": {
        "id": "vy1H-eYUHz7W"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del pre_model\n",
        "del image_feature\n",
        "del questions\n",
        "del word_embedding\n"
      ],
      "metadata": {
        "id": "lk-X779P0j3a"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epochs in range(15):\n",
        "    print(f\"Epoch {epochs+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_function, optimizer)"
      ],
      "metadata": {
        "id": "Wa6pvVtLN9td",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f52c5f5-3eb6-4b4a-cd5c-ef8379273242"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction"
      ],
      "metadata": {
        "id": "aDIj2e6iPXrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = model(test_question_embedding, torch.tensor(test_image_feature))\n",
        "\n",
        "output = np.array([torch.argmax(p).item() for p in prediction], dtype='int64')\n",
        "labeldict = {}\n",
        "labeldict['question_id'] = list(df['question_id'])\n",
        "labeldict['label'] = []\n",
        "for idx, out in enumerate(prediction):\n",
        "  labeldict['label'].append(int(out))\n",
        "\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "-KVrBVQnPZo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), '/content/drive/My Drive/minivqaiust/minivqa-v1-weights.pth')"
      ],
      "metadata": {
        "id": "nxgMAmvBQ9n_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}